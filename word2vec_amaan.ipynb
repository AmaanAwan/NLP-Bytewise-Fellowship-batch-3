{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WIFDxrSVM2"
      },
      "source": [
        "In this notebook, we explore the model proposed by Mikolov et al. in [1]. We will build the Skipgram and CBOW models from scratch, train them on a relatively small corpus, and take a closer look at some analogies using these trained models. We will look at three different number of dimensions of the word embeddings in order to get a better intuition how the number of dimensions influences the result. The goal is not to obtain a high performance. Rather, the goal is to get a better understanding of the models. For that reason, Skipgram does not use negative sampling even though it would be used in practice.\n",
        "\n",
        "\n",
        "[1] Mikolov, Tomas, et al. \"Efficient Estimation of Word Representations in Vector Space\" Advances in neural information processing systems. 2013."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VOelR7BYQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ede3933-1be1-41f8-f76a-5aa0d900cfe9"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCnATRPgBZEd"
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors as nn\n",
        "from matplotlib import pylab\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCd0zUO1AKjY"
      },
      "source": [
        "### Import file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz8Z5gCDBhSl"
      },
      "source": [
        "file_name = '/content/alice.txt'\n",
        "corpus = open(file_name).readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkbk32wHANnD"
      },
      "source": [
        "### Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37PyOHq2BkY4"
      },
      "source": [
        "# Remove sentences with fewer than 3 words\n",
        "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
        "\n",
        "# Remove punctuation in text and fit tokenizer on entire corpus\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Convert text to sequence of integer values\n",
        "corpus = tokenizer.texts_to_sequences(corpus)\n",
        "n_samples = sum(len(s) for s in corpus) # Total number of words in the corpus\n",
        "V = len(tokenizer.word_index) + 1 # Total number of unique words in the corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILdA_IimBlte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6d3e45-1a8c-4c87-d6cd-bcf32200ec96"
      },
      "source": [
        "n_samples, V"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27165, 2557)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRbpue0WBms6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ce82e8-cbda-452a-de55-7f2da5d68662"
      },
      "source": [
        "# Example of how word to integer mapping looks like in the tokenizer\n",
        "print(list((tokenizer.word_index.items()))[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er86VxH9BqI9"
      },
      "source": [
        "# Parameters\n",
        "window_size = 2\n",
        "window_size_corpus = 4\n",
        "\n",
        "# Set numpy seed for reproducible results\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0sU1JORATvX"
      },
      "source": [
        "## Skipgram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Udp1xKcDBu0v"
      },
      "source": [
        "# Prepare data for the skipgram model\n",
        "def generate_data_skipgram(corpus, window_size, V):\n",
        "    maxlen = window_size * 2\n",
        "    all_in = []\n",
        "    all_out = []\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            p = index - window_size\n",
        "            n = index + window_size + 1\n",
        "\n",
        "            in_words = []\n",
        "            labels = []\n",
        "            for i in range(p, n):\n",
        "                if i != index and 0 <= i < L:\n",
        "                    # Add the input word\n",
        "                    all_in.append(word)\n",
        "                    # Add one-hot of the context words\n",
        "                    all_out.append(to_categorical(words[i], V))\n",
        "\n",
        "    return (np.array(all_in), np.array(all_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA9BcRTZ8FM6"
      },
      "source": [
        "We break down each (target word, context word**s**) pair into (target word, context word) pairs. This is done with the `generate_data_skipgram` method above. This method returns two NumPy arrays: `x` (input, i.e., target word) and `y` (output, i.e., context word). We can now use this method to generate our training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXBGNrKTB0tO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53e8de9-9a30-4979-8651-0aef8e303e2b"
      },
      "source": [
        "# Create training data\n",
        "X_skip, y_skip = generate_data_skipgram(corpus, window_size, V)\n",
        "X_skip.shape, y_skip.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((94556,), (94556, 2557))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1FTkStoDLFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3f8735-0a2d-4439-b593-3af24fde858c"
      },
      "source": [
        "# Create skipgram architecture\n",
        "dims = [50, 150, 300]\n",
        "skipgram_models = []\n",
        "\n",
        "for dim in dims:\n",
        "    # Initialize a Keras Sequential model\n",
        "    skipgram = Sequential()\n",
        "\n",
        "    # Add an Embedding layer\n",
        "    skipgram.add(Embedding(input_dim=V,\n",
        "                           output_dim=dim,\n",
        "                           input_length=1,\n",
        "                           embeddings_initializer='glorot_uniform'))\n",
        "\n",
        "    # Add a Reshape layer, which reshapes the output of the embedding layer (1,dim) to (dim,)\n",
        "    skipgram.add(Reshape((dim, )))\n",
        "\n",
        "    # Add a final Dense layer with the same size as in [1]\n",
        "    skipgram.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    # Compile the model with a suitable loss function and select an optimizer.\n",
        "    # Optimizer Adagrad was used in paper\n",
        "    skipgram.compile(optimizer=keras.optimizers.Adam(),\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "    skipgram.summary()\n",
        "    print(\"\")\n",
        "    skipgram_models.append(skipgram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 50)             127850    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2557)              130407    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258257 (1008.82 KB)\n",
            "Trainable params: 258257 (1008.82 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1, 150)            383550    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 150)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2557)              386107    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 769657 (2.94 MB)\n",
            "Trainable params: 769657 (2.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1, 300)            767100    \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 300)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2557)              769657    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1536757 (5.86 MB)\n",
            "Trainable params: 1536757 (5.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hxaiIYg99q6"
      },
      "source": [
        "We create a list that stores all dimensions (50, 150, and 300). We iterate over this list and create models for each dimension. Note that the base model we created is equal to the model used in Practical 3.1.\n",
        "\n",
        "Since we want to predict a context word using a single input word, our `input_length` equals 1. The input dimension simply equals the size of our vocabulary, namely `V`. This is because we use a one-hot encoding. In this one-hot encoding, the index that represents the input word equals 1, and all other indices equal 0. Since we have to consider all words in our vocabulary, the one-hot encoding is of dimension `V`. We do not need to perform the one-hot encoding ourselves for the input data since this is done by the embedding layer in our model.\n",
        "\n",
        "Since we want to predict a certain context word, which can have `V` possible outcomes, we use a Softmax layer with `V` units such that we can map a probability to each word in `V`. Since our problem is a typical multiclass classification problem, we use **categorical** cross-entropy as our loss function.\n",
        "\n",
        "We use `adam` as our optimizer, which is an adaptive learning rate optimization algorithm that has been designed specifically for training deep neural networks [2]. We have tried various optimizers, such as SGD, RMSProp, Adam, and Adagrad. Adam seemed to perform the best with respect to the loss and accuracy. We also tried varying the learning rate. We initialize the weights with values from a `glorot_uniform` distribution since we were also requested to use this initializer in Practical 3.1, which tackled a very similar problem compared to the one we are trying to solve in this assignment.\n",
        "\n",
        "[2] https://arxiv.org/pdf/1412.6980.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9eg0xPoDP9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca1452f-dc75-4af3-ca56-01e61c3e4e47"
      },
      "source": [
        "# Training the skipgram models\n",
        "for skipgram in skipgram_models:\n",
        "    skipgram.fit(X_skip, y_skip, batch_size=64, epochs=13, verbose=1)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n",
            "1478/1478 [==============================] - 12s 7ms/step - loss: 6.4270 - accuracy: 0.0589\n",
            "Epoch 2/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.8844 - accuracy: 0.0665\n",
            "Epoch 3/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.7558 - accuracy: 0.0773\n",
            "Epoch 4/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.6577 - accuracy: 0.0855\n",
            "Epoch 5/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.5709 - accuracy: 0.0911\n",
            "Epoch 6/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.4921 - accuracy: 0.0968\n",
            "Epoch 7/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.4197 - accuracy: 0.1001\n",
            "Epoch 8/13\n",
            "1478/1478 [==============================] - 8s 5ms/step - loss: 5.3517 - accuracy: 0.1025\n",
            "Epoch 9/13\n",
            "1478/1478 [==============================] - 9s 6ms/step - loss: 5.2879 - accuracy: 0.1040\n",
            "Epoch 10/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.2275 - accuracy: 0.1054\n",
            "Epoch 11/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.1707 - accuracy: 0.1057\n",
            "Epoch 12/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.1179 - accuracy: 0.1065\n",
            "Epoch 13/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.0686 - accuracy: 0.1066\n",
            "\n",
            "Epoch 1/13\n",
            "1478/1478 [==============================] - 12s 8ms/step - loss: 6.2682 - accuracy: 0.0646\n",
            "Epoch 2/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.7325 - accuracy: 0.0825\n",
            "Epoch 3/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.5696 - accuracy: 0.0915\n",
            "Epoch 4/13\n",
            "1478/1478 [==============================] - 7s 4ms/step - loss: 5.4260 - accuracy: 0.0983\n",
            "Epoch 5/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.2908 - accuracy: 0.1024\n",
            "Epoch 6/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.1653 - accuracy: 0.1041\n",
            "Epoch 7/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.0515 - accuracy: 0.1040\n",
            "Epoch 8/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.9498 - accuracy: 0.1043\n",
            "Epoch 9/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.8598 - accuracy: 0.1024\n",
            "Epoch 10/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.7811 - accuracy: 0.1013\n",
            "Epoch 11/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.7139 - accuracy: 0.1014\n",
            "Epoch 12/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.6558 - accuracy: 0.0996\n",
            "Epoch 13/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.6070 - accuracy: 0.0989\n",
            "\n",
            "Epoch 1/13\n",
            "1478/1478 [==============================] - 11s 7ms/step - loss: 6.1815 - accuracy: 0.0692\n",
            "Epoch 2/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.6367 - accuracy: 0.0887\n",
            "Epoch 3/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.4246 - accuracy: 0.0973\n",
            "Epoch 4/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 5.2312 - accuracy: 0.1020\n",
            "Epoch 5/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 5.0591 - accuracy: 0.1019\n",
            "Epoch 6/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.9142 - accuracy: 0.0998\n",
            "Epoch 7/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.7940 - accuracy: 0.0978\n",
            "Epoch 8/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.6991 - accuracy: 0.0968\n",
            "Epoch 9/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.6261 - accuracy: 0.0942\n",
            "Epoch 10/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.5731 - accuracy: 0.0932\n",
            "Epoch 11/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.5342 - accuracy: 0.0934\n",
            "Epoch 12/13\n",
            "1478/1478 [==============================] - 7s 5ms/step - loss: 4.5074 - accuracy: 0.0923\n",
            "Epoch 13/13\n",
            "1478/1478 [==============================] - 6s 4ms/step - loss: 4.4888 - accuracy: 0.0919\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr_d7eqkBkfA"
      },
      "source": [
        "We fit each model that considers a varying number of dimensions with a batch size of 64 and 13 epochs.\n",
        "\n",
        "We observe that the loss decreases very slowly. Moreover, we observe a slowly increasing accuracy. We use accuracy as a metric, so that we can get sort of an idea of when to stop. The bad thing about our approach is that we train all models for an equal number of epochs this way when that is not necessarily the best course of action, but let us not focus too much on this now as the training does not take that long anyway and we just want to create word embeddings. We used an equal number of epochs for all models to make it easier to compare the losses and accuracies of the different models at equal numbers of epochs. After 12 epochs for the model that considers 50 dimensions, we observe that the accuracy does not increase significantly anymore (even decreases a bit during one epoch). This can also be observed for the model with 150 dimensions after epoch 6. For the model with 300 dimensions, we can already stop after 4 epochs since the accuracy only keeps decreasing. Hence, we observe that training these models for more epochs does not necessarily lead to a better accuracy.\n",
        "\n",
        "A major reason for this low accuracy is the corpus size being of relatively small size. Our corpus, namely `alice.txt`, has only 26,283 words before processing the file, which is a lot less than the millions of words that are mentioned by Mikolov et al. in [1]. We suspect that this results in our models simply not being able to learn much from our corpus. This is highlighted even more by Mikolov et al. [1] in  tables 2 and 3 in [1]. Since we trained our models on a relatively small corpus, low accuracies were to be expected. Rezaeinia et al. [3] further support this statement.\n",
        "\n",
        "[3] https://arxiv.org/ftp/arxiv/papers/1711/1711.08609.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGEHlkQ4DShO"
      },
      "source": [
        "for skipgram in skipgram_models:\n",
        "    # Save embeddings for vectors of length 50, 150 and 300 using skipgram model\n",
        "    weights = skipgram.get_weights()\n",
        "\n",
        "    # Get the embedding matrix\n",
        "    embedding = weights[0]\n",
        "\n",
        "    # Get word embeddings for each word in the vocabulary, write to file\n",
        "    f = open(f\"vectors_skipgram_{len(embedding[0])}.txt\", \"w\")\n",
        "\n",
        "    # Create columns for the words and the values in the matrix, makes it easier to read as dataframe\n",
        "    columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
        "\n",
        "    # Start writing to the file, start with the column names\n",
        "    f.write(\" \".join(columns))\n",
        "\n",
        "    # Start a new line\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        f.write(word)\n",
        "        f.write(\" \")\n",
        "        f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4z9Lt6pAZEw"
      },
      "source": [
        "## CBOW\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMhI_sWFTXDW"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Prepare the data for the CBOW model\n",
        "def generate_data_cbow(corpus, window_size, V):\n",
        "    all_in = []\n",
        "    all_out = []\n",
        "\n",
        "    # Iterate over all sentences\n",
        "    for sentence in corpus:\n",
        "        L = len(sentence)\n",
        "        for index, word in enumerate(sentence):\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            # Empty list which will store the context words\n",
        "            context_words = []\n",
        "            for i in range(start, end):\n",
        "                # Skip the 'same' word\n",
        "                if i != index:\n",
        "                    # Add a word as a context word if it is within the window size\n",
        "                    if 0 <= i < L:\n",
        "                        context_words.append(sentence[i])\n",
        "                    else:\n",
        "                        # Pad with zero if there are no words\n",
        "                        context_words.append(0)\n",
        "            # Append the list with context words\n",
        "            all_in.append(context_words)\n",
        "\n",
        "            # Add one-hot encoding of the target word\n",
        "            all_out.append(to_categorical(word, V))\n",
        "\n",
        "    return (np.array(all_in), np.array(all_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRKDTZ_iCrIe"
      },
      "source": [
        "For the CBOW model, we generate the training data differently compared to how we did it for the Skipgram model. With the CBOW model, we want to predict words based on their context. We do this by using a window around the word we want to predict. In our code, this window is represented by the `window_size`. All words contained in the window are the context words. Note that if we want to predict the first or final few words of a sentence (depends on the window size), it might be the case that our window reaches the previous or next sentence, respectively. In such a case, the window around the target word is restricted to the words that are in the same sentence. In the code, we solve this by using padding, which ensures that all sequences of context words the same length. For the padding, we simply use a value of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC-Rvi9uVO5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a453dbb-fc4d-4294-dc46-8a257ae4ca6c"
      },
      "source": [
        "# Create the training data\n",
        "X_cbow, y_cbow = generate_data_cbow(corpus, window_size, V)\n",
        "X_cbow.shape, y_cbow.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27165, 4), (27165, 2557))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAu_XWiVVSZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ad0b06-b4e9-4854-a961-c01a53f081f0"
      },
      "source": [
        "# Create the CBOW architecture\n",
        "cbow_models = []\n",
        "\n",
        "for dim in dims:\n",
        "    cbow = Sequential()\n",
        "\n",
        "    # Add an Embedding layer\n",
        "    cbow.add(Embedding(input_dim=V,\n",
        "                       output_dim=dim,\n",
        "                       input_length=window_size*2, # Note that we now have 2L words for each input entry\n",
        "                       embeddings_initializer='glorot_uniform'))\n",
        "\n",
        "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim, )))\n",
        "\n",
        "    cbow.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "    cbow.compile(optimizer=keras.optimizers.Adam(),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    cbow.summary()\n",
        "    print(\"\")\n",
        "    cbow_models.append(cbow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 4, 50)             127850    \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 50)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2557)              130407    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 258257 (1008.82 KB)\n",
            "Trainable params: 258257 (1008.82 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 4, 150)            383550    \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 150)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2557)              386107    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 769657 (2.94 MB)\n",
            "Trainable params: 769657 (2.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 4, 300)            767100    \n",
            "                                                                 \n",
            " lambda_2 (Lambda)           (None, 300)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2557)              769657    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1536757 (5.86 MB)\n",
            "Trainable params: 1536757 (5.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0pB4Q6UE3h9"
      },
      "source": [
        "Again, we create a list for all the dimensions (50, 150, and 300). We iterate over this list and create models for each dimension.\n",
        "\n",
        "Since we want to predict a target word using `window_size` $\\cdot$ 2 context words, our `input_length` equals `window_size` $\\cdot$ 2. Again, the input dimension simply equals the size of our vocabulary, namely `V`. The reasoning for this is analogous to the reasoning we provided for this matter when discussing the Skipgram model. This reasoning also applies to why we use Softmax and categorical cross-entropy. Furthermore, `adam` is used as our optimizer for the same reasons given in this section for the Skipgram model. Similar reasoning applies to why we use `glorot_uniform` initializers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rlM4jspVVmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0a414a-a03e-400a-f147-03f6d33e87af"
      },
      "source": [
        "# Train CBOW model\n",
        "for cbow in cbow_models:\n",
        "    cbow.fit(X_cbow, y_cbow, batch_size=64, epochs=50, verbose=1)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "425/425 [==============================] - 9s 20ms/step - loss: 6.8223 - accuracy: 0.0586\n",
            "Epoch 2/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 6.0525 - accuracy: 0.0604\n",
            "Epoch 3/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 5.9547 - accuracy: 0.0604\n",
            "Epoch 4/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 5.8718 - accuracy: 0.0617\n",
            "Epoch 5/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 5.7626 - accuracy: 0.0713\n",
            "Epoch 6/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.6401 - accuracy: 0.0858\n",
            "Epoch 7/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.5247 - accuracy: 0.0959\n",
            "Epoch 8/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.4125 - accuracy: 0.1095\n",
            "Epoch 9/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.2994 - accuracy: 0.1234\n",
            "Epoch 10/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 5.1859 - accuracy: 0.1391\n",
            "Epoch 11/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.0757 - accuracy: 0.1496\n",
            "Epoch 12/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.9691 - accuracy: 0.1601\n",
            "Epoch 13/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.8669 - accuracy: 0.1696\n",
            "Epoch 14/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.7696 - accuracy: 0.1794\n",
            "Epoch 15/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.6768 - accuracy: 0.1888\n",
            "Epoch 16/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 4.5881 - accuracy: 0.1968\n",
            "Epoch 17/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.5030 - accuracy: 0.2048\n",
            "Epoch 18/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 4.4215 - accuracy: 0.2122\n",
            "Epoch 19/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.3425 - accuracy: 0.2195\n",
            "Epoch 20/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.2660 - accuracy: 0.2269\n",
            "Epoch 21/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.1917 - accuracy: 0.2331\n",
            "Epoch 22/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 4.1194 - accuracy: 0.2401\n",
            "Epoch 23/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.0493 - accuracy: 0.2474\n",
            "Epoch 24/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.9808 - accuracy: 0.2539\n",
            "Epoch 25/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.9138 - accuracy: 0.2618\n",
            "Epoch 26/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.8480 - accuracy: 0.2685\n",
            "Epoch 27/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.7834 - accuracy: 0.2759\n",
            "Epoch 28/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 3.7207 - accuracy: 0.2825\n",
            "Epoch 29/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.6592 - accuracy: 0.2890\n",
            "Epoch 30/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.5985 - accuracy: 0.2974\n",
            "Epoch 31/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.5395 - accuracy: 0.3050\n",
            "Epoch 32/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.4813 - accuracy: 0.3113\n",
            "Epoch 33/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 3.4245 - accuracy: 0.3190\n",
            "Epoch 34/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 3.3687 - accuracy: 0.3249\n",
            "Epoch 35/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.3145 - accuracy: 0.3338\n",
            "Epoch 36/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.2610 - accuracy: 0.3420\n",
            "Epoch 37/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.2089 - accuracy: 0.3489\n",
            "Epoch 38/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.1582 - accuracy: 0.3557\n",
            "Epoch 39/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.1079 - accuracy: 0.3637\n",
            "Epoch 40/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.0591 - accuracy: 0.3725\n",
            "Epoch 41/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.0114 - accuracy: 0.3778\n",
            "Epoch 42/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.9648 - accuracy: 0.3844\n",
            "Epoch 43/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.9192 - accuracy: 0.3914\n",
            "Epoch 44/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.8746 - accuracy: 0.3985\n",
            "Epoch 45/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.8313 - accuracy: 0.4053\n",
            "Epoch 46/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.7888 - accuracy: 0.4128\n",
            "Epoch 47/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.7475 - accuracy: 0.4204\n",
            "Epoch 48/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.7073 - accuracy: 0.4283\n",
            "Epoch 49/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.6680 - accuracy: 0.4356\n",
            "Epoch 50/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.6292 - accuracy: 0.4426\n",
            "\n",
            "Epoch 1/50\n",
            "425/425 [==============================] - 9s 21ms/step - loss: 6.6188 - accuracy: 0.0599\n",
            "Epoch 2/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 5.9107 - accuracy: 0.0637\n",
            "Epoch 3/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.6770 - accuracy: 0.0864\n",
            "Epoch 4/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.4614 - accuracy: 0.1109\n",
            "Epoch 5/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.2525 - accuracy: 0.1352\n",
            "Epoch 6/50\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 5.0433 - accuracy: 0.1594\n",
            "Epoch 7/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.8440 - accuracy: 0.1767\n",
            "Epoch 8/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 4.6573 - accuracy: 0.1955\n",
            "Epoch 9/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 4.4819 - accuracy: 0.2112\n",
            "Epoch 10/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.3156 - accuracy: 0.2257\n",
            "Epoch 11/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.1582 - accuracy: 0.2382\n",
            "Epoch 12/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 4.0068 - accuracy: 0.2546\n",
            "Epoch 13/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.8613 - accuracy: 0.2687\n",
            "Epoch 14/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.7207 - accuracy: 0.2830\n",
            "Epoch 15/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 3.5845 - accuracy: 0.2991\n",
            "Epoch 16/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.4528 - accuracy: 0.3157\n",
            "Epoch 17/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.3261 - accuracy: 0.3313\n",
            "Epoch 18/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 3.2042 - accuracy: 0.3467\n",
            "Epoch 19/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 3.0870 - accuracy: 0.3635\n",
            "Epoch 20/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.9730 - accuracy: 0.3813\n",
            "Epoch 21/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.8648 - accuracy: 0.3986\n",
            "Epoch 22/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.7608 - accuracy: 0.4159\n",
            "Epoch 23/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.6613 - accuracy: 0.4320\n",
            "Epoch 24/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 2.5660 - accuracy: 0.4505\n",
            "Epoch 25/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.4756 - accuracy: 0.4664\n",
            "Epoch 26/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.3894 - accuracy: 0.4830\n",
            "Epoch 27/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.3078 - accuracy: 0.4990\n",
            "Epoch 28/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.2301 - accuracy: 0.5135\n",
            "Epoch 29/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.1570 - accuracy: 0.5279\n",
            "Epoch 30/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 2.0873 - accuracy: 0.5412\n",
            "Epoch 31/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.0213 - accuracy: 0.5550\n",
            "Epoch 32/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.9593 - accuracy: 0.5647\n",
            "Epoch 33/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.9007 - accuracy: 0.5773\n",
            "Epoch 34/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.8451 - accuracy: 0.5860\n",
            "Epoch 35/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.7928 - accuracy: 0.5965\n",
            "Epoch 36/50\n",
            "425/425 [==============================] - 3s 6ms/step - loss: 1.7432 - accuracy: 0.6050\n",
            "Epoch 37/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.6965 - accuracy: 0.6133\n",
            "Epoch 38/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.6523 - accuracy: 0.6224\n",
            "Epoch 39/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.6109 - accuracy: 0.6294\n",
            "Epoch 40/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.5710 - accuracy: 0.6375\n",
            "Epoch 41/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.5342 - accuracy: 0.6437\n",
            "Epoch 42/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.4987 - accuracy: 0.6501\n",
            "Epoch 43/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.4658 - accuracy: 0.6561\n",
            "Epoch 44/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.4341 - accuracy: 0.6639\n",
            "Epoch 45/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.4041 - accuracy: 0.6692\n",
            "Epoch 46/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.3761 - accuracy: 0.6736\n",
            "Epoch 47/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.3492 - accuracy: 0.6807\n",
            "Epoch 48/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.3239 - accuracy: 0.6847\n",
            "Epoch 49/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 1.3001 - accuracy: 0.6899\n",
            "Epoch 50/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.2772 - accuracy: 0.6954\n",
            "\n",
            "Epoch 1/50\n",
            "425/425 [==============================] - 9s 19ms/step - loss: 6.5102 - accuracy: 0.0612\n",
            "Epoch 2/50\n",
            "425/425 [==============================] - 3s 7ms/step - loss: 5.7342 - accuracy: 0.0830\n",
            "Epoch 3/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.3986 - accuracy: 0.1231\n",
            "Epoch 4/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 5.0911 - accuracy: 0.1559\n",
            "Epoch 5/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 4.8038 - accuracy: 0.1833\n",
            "Epoch 6/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.5397 - accuracy: 0.2069\n",
            "Epoch 7/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 4.2922 - accuracy: 0.2286\n",
            "Epoch 8/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 4.0581 - accuracy: 0.2503\n",
            "Epoch 9/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.8353 - accuracy: 0.2733\n",
            "Epoch 10/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.6212 - accuracy: 0.2975\n",
            "Epoch 11/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.4180 - accuracy: 0.3216\n",
            "Epoch 12/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 3.2251 - accuracy: 0.3461\n",
            "Epoch 13/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 3.0419 - accuracy: 0.3731\n",
            "Epoch 14/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.8697 - accuracy: 0.3978\n",
            "Epoch 15/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.7091 - accuracy: 0.4232\n",
            "Epoch 16/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.5588 - accuracy: 0.4486\n",
            "Epoch 17/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 2.4194 - accuracy: 0.4748\n",
            "Epoch 18/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.2906 - accuracy: 0.5010\n",
            "Epoch 19/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.1719 - accuracy: 0.5218\n",
            "Epoch 20/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 2.0641 - accuracy: 0.5417\n",
            "Epoch 21/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.9642 - accuracy: 0.5616\n",
            "Epoch 22/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.8736 - accuracy: 0.5786\n",
            "Epoch 23/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.7903 - accuracy: 0.5932\n",
            "Epoch 24/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.7144 - accuracy: 0.6060\n",
            "Epoch 25/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.6454 - accuracy: 0.6211\n",
            "Epoch 26/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.5816 - accuracy: 0.6340\n",
            "Epoch 27/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.5245 - accuracy: 0.6461\n",
            "Epoch 28/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.4712 - accuracy: 0.6533\n",
            "Epoch 29/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.4229 - accuracy: 0.6654\n",
            "Epoch 30/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.3779 - accuracy: 0.6733\n",
            "Epoch 31/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.3375 - accuracy: 0.6797\n",
            "Epoch 32/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 1.3005 - accuracy: 0.6890\n",
            "Epoch 33/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.2660 - accuracy: 0.6947\n",
            "Epoch 34/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.2347 - accuracy: 0.7021\n",
            "Epoch 35/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.2051 - accuracy: 0.7084\n",
            "Epoch 36/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.1780 - accuracy: 0.7121\n",
            "Epoch 37/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.1540 - accuracy: 0.7175\n",
            "Epoch 38/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.1308 - accuracy: 0.7210\n",
            "Epoch 39/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 1.1100 - accuracy: 0.7249\n",
            "Epoch 40/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.0900 - accuracy: 0.7301\n",
            "Epoch 41/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.0716 - accuracy: 0.7327\n",
            "Epoch 42/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.0543 - accuracy: 0.7361\n",
            "Epoch 43/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.0379 - accuracy: 0.7414\n",
            "Epoch 44/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 1.0234 - accuracy: 0.7425\n",
            "Epoch 45/50\n",
            "425/425 [==============================] - 2s 6ms/step - loss: 1.0097 - accuracy: 0.7451\n",
            "Epoch 46/50\n",
            "425/425 [==============================] - 2s 5ms/step - loss: 0.9964 - accuracy: 0.7473\n",
            "Epoch 47/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 0.9845 - accuracy: 0.7480\n",
            "Epoch 48/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 0.9731 - accuracy: 0.7501\n",
            "Epoch 49/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 0.9618 - accuracy: 0.7545\n",
            "Epoch 50/50\n",
            "425/425 [==============================] - 2s 4ms/step - loss: 0.9517 - accuracy: 0.7563\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRGtpFspHgq0"
      },
      "source": [
        "**Motivation**. The motivation for training the CBOW models the way we do is equivalent to the motivation given for training the Skipgram models. The only difference between these models and the Skipgram models is that, for these models, we can still see the accuracy increase and loss decrease after several epochs, while this was not the case for the Skipgram models. This is one of the reasons why we train CBOW for more epochs than Skipgram. Furthermore, we can easily increase the number of epochs for these models compared to the ones for Skipgram since CBOW is significantly faster to train than Skipgram. The results of this model together with the Skipgram model will explored further in Task 1.3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGoH2oaEVXPM"
      },
      "source": [
        "for cbow in cbow_models:\n",
        "    # Save embeddings for vectors of length 50, 150 and 300 using cbow model\n",
        "    weights = cbow.get_weights()\n",
        "\n",
        "    # Get the embedding matrix\n",
        "    embedding = weights[0]\n",
        "\n",
        "    # Get word embeddings for each word in the vocabulary, write to file\n",
        "    f = open(f'vectors_cbow_{len(embedding[0])}.txt', 'w')\n",
        "\n",
        "    # Create columns for the words and the values in the matrix, makes it easier to read as dataframe\n",
        "    columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
        "\n",
        "    # Start writing to the file, start with the column names\n",
        "    f.write(\" \".join(columns))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        f.write(word)\n",
        "        f.write(\" \")\n",
        "        f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr2BAI4nAfnG"
      },
      "source": [
        "## Analogy function\n",
        "\n",
        "Let us implement our own function to perform the analogy task. We will use the same distance metric as in [1]. With this function, we want to be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. In a perfect scenario, we would like that this analogy ($e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. In this context, we will call \"man\" the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e., the result of the formula). Then, we can check if $p$ is the same word as the true word $t$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRsO95Nbj-_l"
      },
      "source": [
        "### Computing the distance between the predicted and true word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5VqLZ4KeAF3"
      },
      "source": [
        "def embed(word, embedding, vocab_size=V, tokenizer=tokenizer):\n",
        "    \"\"\" Embed a word by getting the one hot encoding and taking the dot product of this vector with the\n",
        "        embedding matrix 'word' = string type\n",
        "    \"\"\"\n",
        "    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
        "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
        "    # get the one-hot encoding of the word\n",
        "    bin_word = to_categorical(int_word, V)\n",
        "    return np.dot(bin_word, embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_MzogIxeQWl"
      },
      "source": [
        "def compute_distance(word_a, word_b, word_c, word_d):\n",
        "    \"\"\" Returns the cosine distance between the predicted and the true word (word_d)\n",
        "\n",
        "    Our analogy function is: 'word_a is to word_b as word_c is to ?'\n",
        "    Here, ? is predicted based on the embeddings. Then, we compare ? to word_d, which is the true word.\n",
        "    \"\"\"\n",
        "    models = skipgram_models + cbow_models\n",
        "    embeddings = [model.get_weights()[0] for model in models]\n",
        "    for embedding in embeddings:\n",
        "        predicted_embedding = embed(word_b, embedding) - embed(word_a, embedding) + embed(word_c, embedding)\n",
        "        dist_exp_true = cosine_distances(predicted_embedding, embed(word_d, embedding))\n",
        "        print(dist_exp_true[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z867DODghRLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f10a8c3-dc9c-440c-e3d3-2d35b22ba4f4"
      },
      "source": [
        "# Example distances between the predicted and true word for skipgram 50, 100, 150, and cbow 50, 100, 150\n",
        "compute_distance('king', 'queen', 'woman', 'man')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.85569066\n",
            "0.88777417\n",
            "0.8977496\n",
            "0.85658985\n",
            "0.879178\n",
            "0.91641605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWvqcG7kHc5"
      },
      "source": [
        "### Listing the top $z$ closest words based on an analogy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpjL8jA8szkN"
      },
      "source": [
        "from scipy.spatial.distance import cosine, cdist\n",
        "\n",
        "\n",
        "def embed(word, embedding, vocab_size=V, tokenizer=tokenizer):\n",
        "    # Get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
        "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
        "    # Get the one-hot encoding of the word\n",
        "    bin_word = to_categorical(int_word, V)\n",
        "    return np.dot(bin_word, embedding).reshape(-1)\n",
        "\n",
        "\n",
        "def get_nearest_words(model_name, embed_word, used_words, nr=10):\n",
        "    \"\"\"Returns the `nr` nearest words to the `embed_word` for a certain `model_name`\n",
        "    \"\"\"\n",
        "    # Load the model embedding matrix and create a list of all the words\n",
        "    df = pd.read_csv(f\"vectors_{model_name}.txt\", sep=\" \")\n",
        "\n",
        "    # Filter out words that are in the analogy\n",
        "    df = df[~(df[\"word\"].isin(used_words))]\n",
        "\n",
        "    # Store the embedded representation of the words\n",
        "    embedded_words = df.iloc[:, 1:].values\n",
        "    embedded_word = embed_word.reshape(1, -1)\n",
        "\n",
        "    # Get the distances between the input embedding and the embedded words such that we can look for the smallest one\n",
        "    # cdist makes it easy for us to compute the cosine distance between each pair of the two collections of inputs\n",
        "    distances = cdist(embedded_word, embedded_words, \"cosine\").reshape(-1)\n",
        "\n",
        "    # Sort distances and store the indices of the `nr` lowest distances\n",
        "    top_sorted_indices = distances.argsort()[:nr]\n",
        "\n",
        "    # Convert the indices to actual words\n",
        "    top_words = [list(df[\"word\"])[i] for i in top_sorted_indices]\n",
        "\n",
        "    # Keep the rounded values of those indices\n",
        "    values = [round(distances[i], 4) for i in top_sorted_indices]\n",
        "    # Concatenate the top words together with their values and return it as a list\n",
        "    return list(zip(top_words, values))\n",
        "\n",
        "\n",
        "def print_analogy(analogy, embeddings, models, model_names, nr=10):\n",
        "    # Retrieve the words from the analogy we need to compute\n",
        "    word_a, word_b, word_c, word_true = analogy\n",
        "\n",
        "    # Formulate the analogy task\n",
        "    analogy_task = f\"{word_a} is to {word_b} as {word_c} is to ?\"\n",
        "\n",
        "    print(f\"Analogy Task: {analogy_task}\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "\n",
        "    # Iterate over all models available\n",
        "    for model_name, embedding in zip(model_names, embeddings):\n",
        "        # Obtain embeddings for all the words\n",
        "        embed_true = embed(word_true, embedding)\n",
        "        embed_a, embed_b, embed_c = embed(word_a, embedding), embed(word_b, embedding), embed(word_c, embedding)\n",
        "\n",
        "        # Obtain the predicted embedding based on the analogy function\n",
        "        embed_prediction = embed_b - embed_a + embed_c\n",
        "\n",
        "        # The true word with distance similarity value between predicted embedding and true word embedding,\n",
        "        # also denoted `sim1` in the text above\n",
        "        sim1 = round(cosine(embed_true, embed_prediction), 4)\n",
        "\n",
        "        # The predicted word with distance similarity value between predicted embedding and the embedding of the word\n",
        "        # in the vocabulary that is closest to this predicted embedding\n",
        "        word_prediction, sim2 = get_nearest_words(model_name, embed_prediction, [word_a, word_b, word_c], 1)[0]\n",
        "\n",
        "        # Get the top `nr` nearest words\n",
        "        nearest_words = get_nearest_words(model_name, embed_prediction, [word_a, word_b, word_c], nr)\n",
        "\n",
        "        # Print whether or not the true word was in the top nr\n",
        "        partially_correct = word_true in [word[0] for word in nearest_words]\n",
        "\n",
        "        print(f\"Embedding: {model_name}\")\n",
        "        # Print all top nr words with their distance\n",
        "        for word in nearest_words:\n",
        "            print(f\"{word[0]} => {round(word[1], 4)}\")\n",
        "        print(f\"Predicted: {word_prediction} ({round(sim2, 4)}) - True: {word_true} ({sim1})\")\n",
        "        print(f\"Correct? {word_prediction == word_true} - In the top {nr}? {partially_correct}\")\n",
        "        print(\"---------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiXAef0QAe77"
      },
      "source": [
        "The method we have created above is relatively simple. Let us consider the major steps of the method. The method boils down to: 1) concatenating all models such that it is easier to iterate over all models, 2) get the embeddings of each model such that we can easily iterate over them, 3) store the model names in a list such that we can easily iterate over them, 4) create a list of tuples of size four where each word in the tuple represents a word in the analogy, 5) iterate over each tuple in the analogies we want to look at, 6) compute the embedding of each word in the tuple, 7) fill in the analogy function using the first three words, 8) make a prediction based on the outcome of the analogy function and return the `nr` nearest words using the cosine distance (we use this distance measure since it was mentioned in [1]), 9) compare if the actual word (given as input parameter) is equal to the predicted word. This is the main idea behind the method. We have also made it easier to return the top `nr` of nearest words and print the top `nr` nearest words for each prediction together with the cosine distances to give us more of an idea as to what the model is predicting.\n",
        "\n",
        "Important: note that we filter out the words that are in the analogy. That is, for some analogy \"$x$ is to $y$ as $z$ is to ?\", we filter out $x$, $y$, and $z$ from the top `nr` of entries since Mikolov et al. [1] state that they discard the input question words (the words used in the analogy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm3JZBTX_PUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a07cfd-a45c-49eb-b1ff-2453898a6805"
      },
      "source": [
        "# Concatenate all models such that we can easily iterate over all models\n",
        "models = skipgram_models + cbow_models\n",
        "\n",
        "# Store the embeddings of all models such that we can easily iterate over them\n",
        "word_embeddings = [model.get_weights()[0] for model in models]\n",
        "\n",
        "# Store the model names such that we can easily iterate over them\n",
        "model_names = [\"skipgram_50\", \"skipgram_150\", \"skipgram_300\", \"cbow_50\", \"cbow_150\", \"cbow_300\"]\n",
        "\n",
        "# Set the number of top words to print\n",
        "nr = 10\n",
        "\n",
        "print_analogy(analogy=('queen', 'king', 'woman', 'man'), embeddings=word_embeddings, models=models, model_names=model_names, nr=nr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy Task: queen is to king as woman is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "snail => 0.3851\n",
            "vii => 0.4129\n",
            "doze => 0.4504\n",
            "guessed => 0.4739\n",
            "readily => 0.488\n",
            "lory => 0.4897\n",
            "handsome => 0.4925\n",
            "five => 0.4936\n",
            "furious => 0.498\n",
            "steam => 0.5014\n",
            "Predicted: snail (0.3851) - True: man (0.7142)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "unusually => 0.6119\n",
            "childhood => 0.6158\n",
            "eaglet => 0.6328\n",
            "lory => 0.6361\n",
            "odd => 0.6413\n",
            "deeply => 0.6432\n",
            "cautiously => 0.6484\n",
            "snail => 0.6511\n",
            "begin => 0.6548\n",
            "dodo => 0.6619\n",
            "Predicted: unusually (0.6119) - True: man (0.7403)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "eaglet => 0.625\n",
            "m => 0.6408\n",
            "snail => 0.6409\n",
            "readily => 0.6848\n",
            "uncommonly => 0.686\n",
            "unusually => 0.693\n",
            "conger => 0.6959\n",
            "childhood => 0.7022\n",
            "thousand => 0.7059\n",
            "thimble => 0.7097\n",
            "Predicted: eaglet (0.625) - True: man (0.8048)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "snail => 0.3939\n",
            "lory => 0.398\n",
            "thimble => 0.4161\n",
            "cautiously => 0.434\n",
            "m => 0.4366\n",
            "cool => 0.4385\n",
            "tops => 0.4417\n",
            "bird => 0.4501\n",
            "odd => 0.4584\n",
            "doze => 0.4597\n",
            "Predicted: snail (0.3939) - True: man (0.7355)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "m => 0.5723\n",
            "doze => 0.5822\n",
            "thimble => 0.5829\n",
            "odd => 0.5965\n",
            "neatly => 0.6022\n",
            "readily => 0.6067\n",
            "partner => 0.614\n",
            "cautiously => 0.6217\n",
            "snail => 0.6388\n",
            "deeply => 0.6428\n",
            "Predicted: m (0.5723) - True: man (0.7361)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "doze => 0.6583\n",
            "snail => 0.6667\n",
            "station => 0.6712\n",
            "fidgeted => 0.6782\n",
            "readily => 0.6822\n",
            "m => 0.6867\n",
            "cautiously => 0.6931\n",
            "man => 0.7001\n",
            "truthful => 0.7056\n",
            "eaglet => 0.7064\n",
            "Predicted: doze (0.6583) - True: man (0.7001)\n",
            "Correct? False - In the top 10? True\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07nNzi-a_POE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1676e7-9d90-40e8-be5b-eb0ee770b636"
      },
      "source": [
        "analogies = [('he', 'is', 'we', 'are'), ('love', 'hate', 'little', 'large'), ('small', 'smaller', 'large', 'larger'), ('man', 'woman', 'king', 'queen'), ('mouse', 'mice', 'cat', 'cats')]\n",
        "for analogy in analogies:\n",
        "    print_analogy(analogy=analogy, embeddings=word_embeddings, models=models, model_names=model_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analogy Task: he is to is as we is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "size => 0.5029\n",
            "barrowful => 0.5464\n",
            "knowing => 0.5502\n",
            "delightful => 0.5512\n",
            "pity => 0.5528\n",
            "chatte => 0.5558\n",
            "hold => 0.5713\n",
            "flame => 0.5719\n",
            "say => 0.5723\n",
            "mean => 0.574\n",
            "Predicted: size (0.5029) - True: are (0.8379)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "pretending => 0.633\n",
            "oh => 0.642\n",
            "four => 0.6506\n",
            "chuckled => 0.6753\n",
            "cleared => 0.6834\n",
            "whenever => 0.6861\n",
            "used => 0.6903\n",
            "lullaby => 0.6962\n",
            "delightful => 0.6978\n",
            "nurse => 0.7029\n",
            "Predicted: pretending (0.633) - True: are (0.9271)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "chuckled => 0.694\n",
            "cheshire => 0.7195\n",
            "used => 0.7348\n",
            "shore => 0.7435\n",
            "longed => 0.7436\n",
            "barrowful => 0.7496\n",
            "oh => 0.7505\n",
            "pretending => 0.7512\n",
            "nine => 0.7513\n",
            "poker => 0.7567\n",
            "Predicted: chuckled (0.694) - True: are (0.916)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "chuckled => 0.4388\n",
            "means => 0.4793\n",
            "wonderful => 0.4834\n",
            "fills => 0.4995\n",
            "delightful => 0.5025\n",
            "ladder => 0.5051\n",
            "hopeless => 0.5171\n",
            "knowing => 0.5175\n",
            "violence => 0.5179\n",
            "matters => 0.5189\n",
            "Predicted: chuckled (0.4388) - True: are (0.7939)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "oh => 0.6306\n",
            "chuckled => 0.6427\n",
            "grinned => 0.649\n",
            "putting => 0.6684\n",
            "delightful => 0.6812\n",
            "flung => 0.6896\n",
            "means => 0.6908\n",
            "matters => 0.6912\n",
            "repeat => 0.6947\n",
            "fills => 0.6966\n",
            "Predicted: oh (0.6306) - True: are (0.927)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "poured => 0.7165\n",
            "taller => 0.7254\n",
            "doors => 0.726\n",
            "worry => 0.7326\n",
            "chuckled => 0.7364\n",
            "putting => 0.7391\n",
            "since => 0.7427\n",
            "oh => 0.7469\n",
            "grinned => 0.748\n",
            "giving => 0.7486\n",
            "Predicted: poured (0.7165) - True: are (0.8822)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Analogy Task: love is to hate as little is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "saucepans => 0.4201\n",
            "word => 0.4747\n",
            "ignorant => 0.4869\n",
            "expecting => 0.4932\n",
            "key => 0.4963\n",
            "welcome => 0.4995\n",
            "horse => 0.5181\n",
            "scale => 0.5183\n",
            "creature => 0.5193\n",
            "pulled => 0.5221\n",
            "Predicted: saucepans (0.4201) - True: large (0.642)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "unlocking => 0.5909\n",
            "knocked => 0.6229\n",
            "welcome => 0.6556\n",
            "rock => 0.6559\n",
            "golden => 0.6596\n",
            "tiny => 0.6632\n",
            "australia => 0.6681\n",
            "pairs => 0.6778\n",
            "walrus => 0.684\n",
            "fork => 0.6847\n",
            "Predicted: unlocking (0.5909) - True: large (0.882)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "unlocking => 0.6122\n",
            "golden => 0.6483\n",
            "rock => 0.6546\n",
            "australia => 0.6571\n",
            "knocked => 0.6753\n",
            "driest => 0.6761\n",
            "welcome => 0.6826\n",
            "scale => 0.6844\n",
            "horse => 0.6849\n",
            "shepherd => 0.686\n",
            "Predicted: unlocking (0.6122) - True: large (0.8729)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "unlocking => 0.4605\n",
            "bright => 0.4824\n",
            "kissed => 0.4868\n",
            "bottle => 0.4924\n",
            "hiss => 0.5142\n",
            "knocked => 0.5304\n",
            "saucepans => 0.5338\n",
            "crowd => 0.5372\n",
            "tiny => 0.5416\n",
            "uncomfortably => 0.5523\n",
            "Predicted: unlocking (0.4605) - True: large (0.752)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "kissed => 0.5091\n",
            "horse => 0.6274\n",
            "unlocking => 0.6357\n",
            "knocked => 0.6384\n",
            "loud => 0.6438\n",
            "sizes => 0.6526\n",
            "bats => 0.6539\n",
            "pulled => 0.6572\n",
            "driest => 0.6597\n",
            "declare => 0.6629\n",
            "Predicted: kissed (0.5091) - True: large (0.7901)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "kick => 0.6372\n",
            "horse => 0.6424\n",
            "kissed => 0.6519\n",
            "stool => 0.6728\n",
            "unlocking => 0.6733\n",
            "bats => 0.6781\n",
            "knock => 0.6807\n",
            "confusing => 0.6826\n",
            "comfits => 0.683\n",
            "knocked => 0.6864\n",
            "Predicted: kick (0.6372) - True: large (0.9102)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Analogy Task: small is to smaller as large is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "growl => 0.431\n",
            "chains => 0.4355\n",
            "bitter => 0.4422\n",
            "panting => 0.4481\n",
            "burning => 0.4641\n",
            "hate => 0.4664\n",
            "shiver => 0.4717\n",
            "arrived => 0.4736\n",
            "wings => 0.4836\n",
            "wife => 0.4899\n",
            "Predicted: growl (0.431) - True: larger (0.5803)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "sour => 0.5835\n",
            "raw => 0.5911\n",
            "bitter => 0.5934\n",
            "makes => 0.5991\n",
            "giddy => 0.6188\n",
            "known => 0.6247\n",
            "sorts => 0.6367\n",
            "sell => 0.6471\n",
            "grow => 0.6488\n",
            "larger => 0.6502\n",
            "Predicted: sour (0.5835) - True: larger (0.6502)\n",
            "Correct? False - In the top 10? True\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "larger => 0.6173\n",
            "throat => 0.6966\n",
            "bitter => 0.6972\n",
            "makes => 0.7035\n",
            "brought => 0.7136\n",
            "sour => 0.7141\n",
            "music => 0.7144\n",
            "respectable => 0.7147\n",
            "brown => 0.7153\n",
            "frighten => 0.7199\n",
            "Predicted: larger (0.6173) - True: larger (0.6173)\n",
            "Correct? True - In the top 10? True\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "bitter => 0.3974\n",
            "giddy => 0.4875\n",
            "appearing => 0.4888\n",
            "twelve => 0.5001\n",
            "paws => 0.5056\n",
            "barley => 0.5069\n",
            "stigand => 0.5131\n",
            "sorrows => 0.5168\n",
            "books => 0.5228\n",
            "reaching => 0.5255\n",
            "Predicted: bitter (0.3974) - True: larger (0.6396)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "barley => 0.6109\n",
            "flamingoes => 0.6138\n",
            "ourselves => 0.6369\n",
            "hearth => 0.6377\n",
            "tillie => 0.6388\n",
            "stiff => 0.6467\n",
            "straightened => 0.6496\n",
            "legs => 0.6526\n",
            "pour => 0.6547\n",
            "muscular => 0.6586\n",
            "Predicted: barley (0.6109) - True: larger (0.7266)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "brought => 0.6225\n",
            "muscular => 0.6226\n",
            "respectable => 0.6345\n",
            "sour => 0.6466\n",
            "bitter => 0.6489\n",
            "wind => 0.6599\n",
            "flamingoes => 0.6757\n",
            "tillie => 0.6849\n",
            "makes => 0.6855\n",
            "theirs => 0.6901\n",
            "Predicted: brought (0.6225) - True: larger (0.7327)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Analogy Task: man is to woman as king is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "neatly => 0.4264\n",
            "dormouse => 0.4317\n",
            "far => 0.4364\n",
            "fluttered => 0.4366\n",
            "owl => 0.4426\n",
            "leaders => 0.4443\n",
            "she => 0.4481\n",
            "breeze => 0.449\n",
            "tomorrow => 0.4571\n",
            "pencils => 0.458\n",
            "Predicted: neatly (0.4264) - True: queen (0.6283)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "neatly => 0.5765\n",
            "childhood => 0.5837\n",
            "laugh => 0.6138\n",
            "leaders => 0.6177\n",
            "owl => 0.6228\n",
            "she => 0.6432\n",
            "brush => 0.6506\n",
            "arranged => 0.651\n",
            "fortunately => 0.6569\n",
            "odd => 0.6576\n",
            "Predicted: neatly (0.5765) - True: queen (0.7645)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "childhood => 0.5935\n",
            "neatly => 0.6188\n",
            "tiptoe => 0.6615\n",
            "spell => 0.6748\n",
            "eaglet => 0.6977\n",
            "laughing => 0.6979\n",
            "rightly => 0.7025\n",
            "laugh => 0.7026\n",
            "owl => 0.7072\n",
            "fidgeted => 0.7102\n",
            "Predicted: childhood (0.5935) - True: queen (0.8004)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "tide => 0.2969\n",
            "thimble => 0.3257\n",
            "breeze => 0.3496\n",
            "dormouse => 0.3541\n",
            "daisies => 0.3619\n",
            "odd => 0.3755\n",
            "owl => 0.3918\n",
            "spot => 0.4144\n",
            "floor => 0.418\n",
            "second => 0.4245\n",
            "Predicted: tide (0.2969) - True: queen (0.588)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "neatly => 0.4613\n",
            "odd => 0.5006\n",
            "leaders => 0.5403\n",
            "childhood => 0.5712\n",
            "tide => 0.5728\n",
            "thimble => 0.5781\n",
            "owl => 0.5789\n",
            "moral => 0.5923\n",
            "readily => 0.6029\n",
            "normans => 0.6145\n",
            "Predicted: neatly (0.4613) - True: queen (0.7693)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "odd => 0.591\n",
            "tide => 0.615\n",
            "childhood => 0.6316\n",
            "velvet => 0.6394\n",
            "thimble => 0.6422\n",
            "readily => 0.6432\n",
            "owl => 0.6446\n",
            "neatly => 0.6541\n",
            "leaders => 0.6582\n",
            "fidgeted => 0.6659\n",
            "Predicted: odd (0.591) - True: queen (0.7244)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Analogy Task: mouse is to mice as cat is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_50\n",
            "world => 0.4616\n",
            "hearing => 0.4674\n",
            "night => 0.4743\n",
            "sense => 0.4803\n",
            "stockings => 0.4942\n",
            "shakespeare => 0.4962\n",
            "history => 0.498\n",
            "knock => 0.5021\n",
            "rosetree => 0.5059\n",
            "messages => 0.5094\n",
            "Predicted: world (0.4616) - True: cats (0.9267)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_150\n",
            "judge => 0.5957\n",
            "mistake => 0.6217\n",
            "ordered => 0.6313\n",
            "catching => 0.6315\n",
            "shouldn => 0.6387\n",
            "foolish => 0.6447\n",
            "minding => 0.65\n",
            "sir => 0.652\n",
            "shingle => 0.6564\n",
            "knock => 0.6565\n",
            "Predicted: judge (0.5957) - True: cats (0.9225)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_300\n",
            "mistake => 0.6216\n",
            "shingle => 0.6404\n",
            "judge => 0.6489\n",
            "knock => 0.6499\n",
            "lap => 0.6546\n",
            "growing => 0.6805\n",
            "rats => 0.6808\n",
            "song => 0.6849\n",
            "ground => 0.6892\n",
            "mabel => 0.691\n",
            "Predicted: mistake (0.6216) - True: cats (0.8692)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_50\n",
            "stockings => 0.4271\n",
            "sooner => 0.4497\n",
            "inches => 0.4629\n",
            "slipped => 0.4995\n",
            "serpents => 0.5168\n",
            "bawled => 0.5192\n",
            "rosetree => 0.5197\n",
            "shan => 0.527\n",
            "australia => 0.5271\n",
            "despair => 0.5315\n",
            "Predicted: stockings (0.4271) - True: cats (0.9413)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_150\n",
            "face => 0.5649\n",
            "aloud => 0.5943\n",
            "coward => 0.5949\n",
            "mistake => 0.5987\n",
            "despair => 0.6046\n",
            "knock => 0.6263\n",
            "judge => 0.6277\n",
            "rats => 0.6283\n",
            "existence => 0.6285\n",
            "window => 0.6382\n",
            "Predicted: face (0.5649) - True: cats (1.0142)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n",
            "Embedding: cbow_300\n",
            "mistake => 0.5864\n",
            "lap => 0.6228\n",
            "pattern => 0.6375\n",
            "puss => 0.6636\n",
            "judge => 0.6695\n",
            "bank => 0.6707\n",
            "custody => 0.6718\n",
            "face => 0.6773\n",
            "given => 0.6799\n",
            "court => 0.6869\n",
            "Predicted: mistake (0.5864) - True: cats (0.9138)\n",
            "Correct? False - In the top 10? False\n",
            "---------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2hVx3IHLgGx"
      },
      "source": [
        "### Different examples of analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xspWTmLIFxQO"
      },
      "source": [
        "Let us consider some of the more interesting analogies. We pick analogies such that the relations in the analogies are syntactically different. This means that we, e.g., do not pick five adjective - comparative analogies (so not small - smaller <-> small - smaller, large - larger <-> clear - clearer, and other analogies of the same kind). We also decided to pick words that occur relatively frequently in the corpus, since we expect our models to perform better on those than on words that have only appeared a single time. Many analogies other than the ones we list here were experimented with; these can be found as comments in the code above. The five categories we cover for this task together with their respective analogies are:\n",
        "\n",
        "**Pronoun to Verb:**\n",
        "He is to is as we is to ...? (are)\n",
        "\n",
        "**Antonyms:**\n",
        "Love is to hate as little is to ...? (large)\n",
        "\n",
        "**Adjective to its Comparative:**\n",
        "Small is to smaller as large is to larger ...? (larger)\n",
        "\n",
        "**Gender structure:**\n",
        "A man is to a woman as a king is to a ...? (queen)\n",
        "\n",
        "**Singular to Plural:**\n",
        "A mouse is to mice as a cat is to ...? (cats)\n",
        "\n",
        "Note that we filter out the words that are in the analogy. That is, for some analogy \"$x$ is to $y$ as $z$ is to ?\", we filter out $x$, $y$, and $z$ from the top `nr` of entries since Mikolov et al. [1] state that they discard the input question words (the words used in the analogy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGodKfkxLqDD"
      },
      "source": [
        "### Comparing the performance on the analogies between the word embeddings and briefly discuss your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofuv_9FmF2Gu"
      },
      "source": [
        "Let us consider the results of using the six models to predict the five analogies above. The results we obtained are presented in the table below. Note that the annotated number in the \"True word\" and \"Predicted word\" columns represents the distance between the predicted embedding based on the analogy function and the embedding of the word annotated next to the number (this is the nearest word with respect to the cosine distance). Furthermore, note that this number has been rounded at four decimal places.\n",
        "\n",
        "| Analogy task | True word  | Predicted word | Embedding | Correct?|\n",
        "|------|------|------|------|------|\n",
        "|  He is to is as we is to ?  | are (0.7187) | do (0.4837) | SG_50 | False|\n",
        "|  He is to is as we is to ?   | are (0.9673) | used (0.6482)  | SG_150 | False|\n",
        "|  He is to is as we is to ?   | are (0.8766) | longed (0.6806) | SG_300 | False|\n",
        "|  He is to is as we is to ? | are (0.9965) | knowing (0.4759) | CBOW_50 | False|\n",
        "|  He is to is as we is to ?   | are (0.9177) | jogged (0.6363) | CBOW_150 | False|\n",
        "|  He is to is as we is to ?   | are (0.9072) | guessed (0.7075) | CBOW_300 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.7033) | horse (0.4196) | SG_50 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.7775) | unlocking (0.569) | SG_150 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.9304) | rock (0.603) | SG_300 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.6845) | melancholy (0.3903) | CBOW_50 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.943) | kissed (0.486) | CBOW_150 | False|\n",
        "|  Love is to hate as little is to ?     | large (0.9081) | kissed (0.6259)| CBOW_300 | False|\n",
        "| Small is to smaller as large is to ?   | larger (0.5697) | our (0.4819) | SG_50 | False|\n",
        "| Small is to smaller as large is to ?   | larger (0.6191) (third one) | panting (0.6100) | SG_150 | False|\n",
        "| Small is to smaller as large is to ?   |  larger (0.6625) (second one) | giddy (0.6602) | SG_300 | False|\n",
        "| Small is to smaller as large is to ?   | larger (0.6007) | ourselves (0.5046) | CBOW_50 | False|\n",
        "| Small is to smaller as large is to ?   | larger (0.715) | flamingoes (0.6184) | CBOW_150 | False|\n",
        "| Small is to smaller as large is to ?   | larger (0.6984) | bitter (0.6398) | CBOW_300 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.5978) | leave (0.4269) | SG_50 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.7337) | childhood (0.4903) | SG_150 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.8092) | childhood (0.5515)  | SG_300 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.5559) | thimble (0.3271) | CBOW_50 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.7842) | neatly (0.4794)  | CBOW_150 | False|\n",
        "|  A man is to a woman as a king is to a ?   | queen (0.737) | odd (0.5122) | CBOW_300 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.7198) | sent (0.4014) | SG_50 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.7134) | mistake (0.5799) | SG_150 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.8525) | seeing (0.6483) | SG_300 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.8261) | existence (0.4654) | CBOW_50 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.9492)| mistake (0.5566) | CBOW_150 | False|\n",
        "| A mouse is to mice as a cat is to ?    | cats (0.9827)| mistake (0.6009)  | CBOW_300 | False|\n",
        "\n",
        "In terms of performance, we observe that none of the six models managed to correctly predict any of the analogies; all predictions were incorrect. The major reason for this concerns the size of our corpus. Our corpus, namely `alice.txt`, has only 26,283 words before processing the file, which is a lot less than the millions of words that are mentioned by Mikolov et al. in [1]. This suggests that our models are simply not able to learn much from our corpus since there is not that much to learn from.\n",
        "\n",
        "Note, however, that for the analogy \"*small* is to *smaller* as *large* is to ? (*larger*)\", SG_150 and SG_300 almost predicted the correct word (the true word was in the top 3 and top 2, respectively). This result can be observed in the output of the code cell above this cell and in the table. Especially SG_300 was extremely close to predicting this word correctly. A possible reason for these almost correct predictions could be due to the fact that this type of analogy is relatively easy to solve compared to some of our other analogies. More specifically, \"small\" and \"smaller\" are very similar because \"smaller\" is simply \"small\" + \"er\", and \"large\" and \"larger\" are very similar because \"larger\" is simply \"large\" + \"r\". While the predictions by the models were not completely correct (these words were not the nearest words), we still thought it would be interesting to mention this fact.\n",
        "\n",
        "Given the poor performance of both models on the analogies (on our corpus), it is tough to say which model performs better. To be able to draw sound conclusions on which type of model is more suitable for predicting analogies, we believe that we would require a larger corpus. If we were to draw a conclusion from the results we have right now, we would argue that Skipgram performs slightly better since it almost predicted the correct word twice, as mentioned in the paragraph above. This is supported by a comment from Mikolov in [4], where he, based on his experience, argues that Skipgram works well with a small amount of the training data and represents rare words or phrases well.\n",
        "\n",
        "From the table, we observe that for the lowest dimension (namely `dim=50`), both types of models are more inclined to predict different words than models with larger dimensions (namely `dim=150` and `dim=300`). See, for example, the analogy \"Love is to hate as little is to ?\", where CBOW_150 and CBOW_300 both predicted \"kissed\", while CBOW_50 predicted \"melancholy\". Another example is the analogy \"A man is to a woman as a king is to a ?\", where SG_150 and SG_300 both predicted \"childhood\", while SG_50 predicted \"leave\". This occurrence could stem from the fact that our corpus size is rather small, which results in us suspecting that there are fewer \"features\" to learn from (e.g., fewer diverse words with different meanings in different contexts) than in much larger corpora. Note that this occurrence only happens three times in total. We thought that it would be something interesting enough to mention in this section.\n",
        "\n",
        "We initially ran these methods without excluding the words that were present in the analogy. When we did not exclude these words, we observed that we often predicted one of the three words that were present in the analogy. Intuitively, this makes sense as these words are used in the input and, therefore, heavily influence the result. Based on these experiments and applying some logic, it makes a lot of sense why Mikolov et al. [1] discard the input question words (the words used in the analogy).\n",
        "\n",
        "[4] https://groups.google.com/forum/#!searchin/word2vec-toolkit/c-bow/word2vec-toolkit/NLvYXU99cAM/E5ld8LcDxlAJ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h-OWIF9AsKH"
      },
      "source": [
        "## Miscellaneous\n",
        "\n",
        "Let us take a closer look at the size of the training data used by Skipgram and CBOW.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRnAJ929PumP"
      },
      "source": [
        "### Which model has more training samples?\n",
        "Let us focus on our corpus first and look at which model has more training samples. After that, we will explore why this is the case from a higher level. Note that in our analysis, we use that $L=2$. The shape of the training samples for CBOW is $(27165, 4)$, which can be seen as $27165$ samples, where each sample contains $4$ (context) words. This means that, for CBOW, the size of the training data equals the total number of words in the corpus.\n",
        "\n",
        "The shape of the training samples for Skipgram is $(94556,)$, which can be seen as $94556$ samples, each containing a single word. This means that, for Skipgram, the size of the training data is slightly less than `n_samples` $\\cdot$ `window_size` $\\cdot 2$ in most cases. The actual size is somewhat less because we cannot capture all words in the full window size for words at the beginning and end of sentences. We will cover this more in the next few paragraphs.\n",
        "\n",
        "Based on the information above, we can clearly conclude that Skipgram has more training samples if both CBOW and Skipgram have the same sentences as input.\n",
        "\n",
        "### Why?\n",
        "Let us consider how the data is generated for each method.\n",
        "\n",
        "In CBOW, we have a training sample for each word we want to predict. This means that the total number of training samples is equal to the total number of words in the corpus. Moreover, each training sample consists of all the words that are contained in a window around the word we want to predict. Let $L$ denote the window size. The total number of words per training sample equals $2L$ after applying padding to make sure each training sample has $2L$ words.\n",
        "\n",
        "In Skipgram, we want to predict the context words based on an input word. In practice, we generated the training samples by specifying a window size $L$ and iterating over all sentences. For each word $w_t$ in the sentence, we add the word pairs $(w_t, w_{t-L}), \\cdots, (w_t, w_{t-1}), (w_t, w_{t+1}), \\cdots (w_t, w_{t+L})$ to the set of training samples. Note that it might not always be possible to make use of the full window size because it might be the case that, for example, at the beginning of a sentence, we cannot consider the $L$ words before the first word of that sentence since these words do not exist in our sentence. (This 'issue' has to do with us considering all words within a certain sentence.) This problem also appears when we consider words at the end of the sentence. This is a reason as to why we have slightly less than the number we proposed above.\n",
        "\n",
        "From these descriptions of the data generation methods for both models, it is evident why Skipgram has more training samples than CBOW for an equal number of sentences with the same content. In conclusion, Skipgram has slightly less than `n_samples` $\\cdot$ `window_size` $\\cdot 2$ samples due to the way the samples are created. On the other hand, CBOW has `n_samples` training samples. Again, the former is larger in size (assuming `window_size` $\\geq 1$)."
      ]
    }
  ]
}